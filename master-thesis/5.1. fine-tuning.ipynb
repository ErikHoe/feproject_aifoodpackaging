{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae19909-273d-4c42-ab2c-fddd571e3256",
   "metadata": {},
   "source": [
    "# 5.1. Fine-Tuning\n",
    "## Dreambooth + LoRa (Campusbier Fine-Tuning)\n",
    "\n",
    "This notebook is based on: \n",
    "\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/SDXL_DreamBooth_LoRA_.ipynb#scrollTo=XUxRrLfLMnBb\n",
    "\n",
    "All fine-tuned models can be found here:\n",
    "\n",
    "https://huggingface.co/erikhsos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61ec4f-0ac0-44ae-b8c6-6a46ca7956b9",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In order to obtain the optimum fine-tuning results,\n",
    "\n",
    "\n",
    "1. **Number of training images**\n",
    "\n",
    "[1, 3, 5, 7, 10, 15, 20, 30]\n",
    "\n",
    "and then the\n",
    "\n",
    "2. **Hyperparameter `Learning rate` and `Training steps`**\n",
    "\n",
    "$5 \\times 10^{-4}$, \n",
    "$1 \\times 10^{-4}$, \n",
    "$5 \\times 10^{-5}$\n",
    "\n",
    "and\n",
    "\n",
    "[500, 1000, 1500, 2000]\n",
    "\n",
    "\n",
    "were evaluated. \n",
    "\n",
    "For each training, **50 output images** were generated, which were rated as “achieved” and “not achieved” in the following categories:\n",
    "\n",
    "1. **brand coherence**\n",
    "2. **target design**\n",
    "3. **visual aesthetics**\n",
    "\n",
    "---\n",
    "\n",
    "**Find the evaluated image-grids under:**\n",
    "\n",
    "anhang/anhang 01_number training images_grids\n",
    "\n",
    "anhang/anhang 02_learning rate + training steps_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac51189-3fa3-4500-902d-c4b01824eb34",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "The **Campusbier-Training** was done with the `instance prompt` consisting of the `unique identifier` and the `subject class`\n",
    "\n",
    "**`\"A [CB] bottle photo\"`**\n",
    "\n",
    "The **Nesquik-Training** was done with\n",
    "\n",
    "**`\"A TOK cereal box photo\"`**\n",
    "\n",
    "-----\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1bf6bf-25a9-4880-ad4b-a46195a96f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/cluster/user/ehoemmen/.cache\n",
    "%env HF_DATASETS_CACHE=/cluster/user/ehoemmen/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af18144-6a3b-4423-bc40-593909a4a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -U diffusers bitsandbytes transformers accelerate peft compel scipy torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08647d0b-8033-45d4-83cc-77efb7ebbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Diffusers Dreambooth x LoRa script\n",
    "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ea9d6-133e-489e-ae96-ec194fd04883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare grid\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols, resize=256):\n",
    "\n",
    "    if resize is not None:\n",
    "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb26e2-00fc-497a-a020-ae406bc12c71",
   "metadata": {},
   "source": [
    " ### Generate custom captions with BLIP\n",
    "\n",
    " Load BLIP to auto caption your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb06b95-2ee3-4e5f-8ef8-06b5390778a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load the processor and the captioning model\n",
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\",cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\",cache_dir=\"/cluster/user/ehoemmen/.cache\",torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# captioning utility\n",
    "def caption_images(input_image):\n",
    "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    pixel_values = inputs.pixel_values\n",
    "\n",
    "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976af1a-6940-4c20-bf4f-3a3213796851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# create a list of (Pil.Image, path) pairs\n",
    "local_dir = \"\"   #enter path\n",
    "imgs_and_paths = [(path,Image.open(path)) for path in glob.glob(f\"{local_dir}*.png\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a46428-505a-43a5-974b-e87762c402cb",
   "metadata": {},
   "source": [
    "Now let's add the concept `token identifier`\n",
    "\n",
    "I chose this combination of `identifier` and `class`:\n",
    "\n",
    "\"[CB] bottle photo \", because \"Campusbier\" contains the word \"beer\" and I want to avoid this having an influence on the training. I also only use \"bottle\". The reason is that I tested the class \"beer bottle \", which resulted in beer cans being generated frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dce3e0-a1f2-4b8f-a042-a864f5540387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "caption_prefix = \"TOK cereal box photo\" #@param\n",
    "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
    "  for img in imgs_and_paths:\n",
    "      caption = caption_prefix + caption_images(img[1]).split(\"\\n\")[0]\n",
    "      entry = {\"file_name\":img[0].split(\"/\")[-1], \"prompt\": caption}\n",
    "      json.dump(entry, outfile)\n",
    "      outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ae0c2-0b97-4bb1-8384-647420b7a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up some memory\n",
    "\n",
    "import gc\n",
    "\n",
    "# delete the BLIP pipelines and free up some memory\n",
    "del blip_processor, blip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c852e6-4ebd-48cd-8b23-6104c01df134",
   "metadata": {},
   "source": [
    "# Preprare for Training\n",
    "\n",
    "Initialize `accelerate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1a5b6-df2c-4dd2-87e5-c2628976e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b548fba6-249b-43b2-a7a5-b5d206a8ebce",
   "metadata": {},
   "source": [
    "### Log into your Hugging Face account\n",
    "Pass [your **write** access token](https://huggingface.co/settings/tokens) so that we can push the trained checkpoints to the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4734b7-910f-43b8-8522-ea04e314c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc6bc0-ded1-4451-8503-52f1152d439b",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Training takes about 1 - 2 hours (depends on the number of `training steps`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eef18c-8292-4130-a538-f4ff8312cca3",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters\n",
    "Für die **Evaluation der Trainingsbilder** wurde folgende Trainingsparamter-Konfiguration verwendet:\n",
    "\n",
    "* `unique identifier`  [CB]\n",
    "* `subject class`   bottle photo\n",
    "* `instance prompt`   A [CB] bottle photo\n",
    "* `learning rate`   0,0004\n",
    "* `training steps`   500\n",
    "* `number training images`   [1, 3, 5, 7, 10, 15, 20, 30]\n",
    "\n",
    "\n",
    "\n",
    "Training will be logged to **weights and biases** and pushed to **huggingface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebbc30-3791-4101-b087-b2ba1504c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
    "  --dataset_name=\"\" \\  #enter path\n",
    "  --output_dir=\"\" \\   #enter path\n",
    "  --caption_column=\"prompt\"\\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --instance_prompt=\"[CB] bottle photo\" \\\n",
    "  --resolution=1024 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=3 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --snr_gamma=5.0 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --max_train_steps=1000 \\\n",
    "  --validation_prompt=\"A pink [CB] bottle photo\" \\\n",
    "  --validation_epochs=50 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --train_text_encoder \\\n",
    "  --seed=\"0\" \\\n",
    "  --push_to_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05636eb9-a691-4303-88d9-4585d26b92bc",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f354b23-be05-4516-9d1c-84ffd54aa0c9",
   "metadata": {},
   "source": [
    "Setup the Pipeline and load your own trained LoRa Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cae862-a3a3-40a1-8f3e-3ac1fb561414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, AutoencoderKL, AutoPipelineForImage2Image\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.load_lora_weights(\"erikhsos/cbbier_06-15-images_LoRA_lr5-4_1500\")\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068974f-eddf-4f23-8507-f510538c9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unload LoRa weights\n",
    "\n",
    "pipe.unload_lora_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0fa3f-7a37-4b18-a130-b51e07763af0",
   "metadata": {},
   "source": [
    "## Create 50 experiment images\n",
    "\n",
    "Change `lora_checkpoint`for specific generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f22e5-eb31-48b4-a21e-3140ac095af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, AutoencoderKL, AutoPipelineForImage2Image\n",
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# Load the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\", \n",
    "    torch_dtype=torch.float16, \n",
    "    cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "# Define the base pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "# List of LoRA checkpoints (number training images)\n",
    "lora_checkpoints = [\n",
    "    \"cbbier_01-1-image_LoRA_lr1-4_500\",\n",
    "    # \"cbbier_02-3-images_LoRA_lr1-4_500\",\n",
    "    # \"cbbier_03-5-images_LoRA_lr1-4_500\",\n",
    "    # \"cbbier_04-7-images_LoRA_lr1-4_500\",\n",
    "    # \"cbbier_05-10-images_LoRA_lr1-4_500\",\n",
    "    # \"cbbier_06-15-images_LoRA_lr1-4_500\",\n",
    "    # \"cbbier_07-20-images_LoRA_lr1-4_500\",\n",
    "    # \"cbbier_08-30-images_LoRa_lr1-4_500\"\n",
    "]\n",
    "\n",
    "# List of LoRA checkpoints (learning rate + training steps)\n",
    "lora_checkpoints = [\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr1-4_500\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr1-4_1000\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr1-4_1500\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr1-4_2000\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-4_500\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-4_1000\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-4_1500\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-4_2000\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-5_500\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-5_1000\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-5_1500\",\n",
    "    # \"erikhsos/cbbier_06-15-images_LoRA_lr5-5_2000\"\n",
    "]\n",
    "\n",
    "# Function to save image grid\n",
    "def save_images(images, folder, base_name, start_index, num_digits):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, img in enumerate(images):\n",
    "        img_path = os.path.join(folder, f\"{base_name}_{str(start_index + i).zfill(num_digits)}.png\")\n",
    "        img.save(img_path)\n",
    "\n",
    "# Function to get the next image index\n",
    "def get_next_image_index(folder, base_name):\n",
    "    existing_images = [f for f in os.listdir(folder) if re.match(rf\"{base_name}_\\d+\\.png\", f)]\n",
    "    if not existing_images:\n",
    "        return 1, 1\n",
    "    max_index = max([int(re.search(rf\"{base_name}_(\\d+)\\.png\", f).group(1)) for f in existing_images])\n",
    "    num_digits = len(str(max_index))\n",
    "    return max_index + 1, num_digits\n",
    "\n",
    "# Generate images for each checkpoint\n",
    "for checkpoint in lora_checkpoints:\n",
    "    # Load LoRA weights\n",
    "    pipe.load_lora_weights(checkpoint)\n",
    "\n",
    "    # Define the prompt and number of images\n",
    "    num_images = 50\n",
    "    \n",
    "    prompt = [\"a [CB] bottle photo with a pink label and the text CAMPUSBIER\"] * num_images\n",
    "    negative_prompt = \"\"\n",
    "\n",
    "    # Define the folder and base name\n",
    "    folder = f\"{checkpoint}_experiment_images\"\n",
    "    base_name = checkpoint\n",
    "\n",
    "    # Get the starting index for image numbering and the number of digits\n",
    "    start_index, num_digits = get_next_image_index(folder, base_name)\n",
    "\n",
    "    # Generate images\n",
    "    images = pipe(prompt=prompt,\n",
    "                  negative_prompt=negative_prompt,\n",
    "                  num_inference_steps=25, \n",
    "                  height=1024, width=1024,\n",
    "                  guidance_scale=4,\n",
    "                 ).images\n",
    "    \n",
    "    # Save images\n",
    "    save_images(images, folder, base_name, start_index, num_digits)\n",
    "\n",
    "    # Unload LoRA weights\n",
    "    pipe.unload_lora_weights()\n",
    "\n",
    "print(\"Image generation and saving completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5983625-73d6-43e1-ba29-fb926165fee2",
   "metadata": {},
   "source": [
    "## Save Images as Grid (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930be04-0c7d-4344-ae7e-e6038a57d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Pfad zu deinem Bildordner\n",
    "image_folder = \"\" #enter path to image folder\n",
    "output_pdf = \"nesquik_rot_dreambooth only_output_grid.pdf\"\n",
    "\n",
    "# Alle Bilddateien im Ordner sammeln\n",
    "image_files = [os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "# Sicherstellen, dass du genau 50 Bilder hast\n",
    "image_files = image_files[:50]\n",
    "\n",
    "# Zielgröße für das Grid (z.B. 5x10)\n",
    "grid_size = (10, 5)\n",
    "\n",
    "# Lade das erste Bild, um die Bildgröße herauszufinden\n",
    "img_sample = Image.open(image_files[0])\n",
    "img_width, img_height = img_sample.size\n",
    "\n",
    "# Berechne die Größe des Gesamtrasters\n",
    "grid_width = grid_size[1] * img_width\n",
    "grid_height = grid_size[0] * img_height\n",
    "\n",
    "# Neues Bild für das Grid erstellen\n",
    "grid_image = Image.new('RGB', (grid_width, grid_height))\n",
    "\n",
    "# Bilder in das Grid einfügen\n",
    "for idx, image_file in enumerate(image_files):\n",
    "    img = Image.open(image_file)\n",
    "    row = idx // grid_size[1]\n",
    "    col = idx % grid_size[1]\n",
    "    grid_image.paste(img, (col * img_width, row * img_height))\n",
    "\n",
    "# Das Grid als PDF speichern\n",
    "grid_image.save(output_pdf, \"PDF\", resolution=300)\n",
    "\n",
    "print(f\"Grid PDF gespeichert als {output_pdf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024cdd0-915e-4543-a182-0e49187902df",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4870fa9-ceb4-4236-9104-b8ae44a7dd99",
   "metadata": {},
   "source": [
    "Best results were archieved with the following hyperparameter configuration:\n",
    "\n",
    "`number of training images` **15**,\n",
    "\n",
    "`learning rate` **$1 \\times 10^{-4}$**, \n",
    "\n",
    "`training steps` **2000**\n",
    "\n",
    "The results still lack quality and text can only rarely be displayed correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
