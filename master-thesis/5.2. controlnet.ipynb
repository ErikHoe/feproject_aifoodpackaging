{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bcef25-7f64-4a76-be8d-05435792f2ce",
   "metadata": {},
   "source": [
    "# 5.2. ControlNet\n",
    "## Dreambooth + ControlNet\n",
    "\n",
    "Controlling image diffusion models by conditioning the model with an additional input image\n",
    "\n",
    "----\n",
    "\n",
    "Based on the **fine-tuning** evaluation (see [**Chapter 5.1. Fine-Tuning**](5.1.%20fine-tuning.ipynb)), **ControlNet** is now  used to **control image generation** more precisely and further **improve image quality** of the food packaging designs.\n",
    "\n",
    "\n",
    "All image generations were done with the following **hyperparameters**:\n",
    "\n",
    "`number of training images` **15**,\n",
    "\n",
    "`learning rate` **$1 \\times 10^{-4}$**, \n",
    "\n",
    "`training steps` **2000**\n",
    "\n",
    "The **dreambooth x controlnet evaluation** was based on 50 output-images, which were rated as “achieved” and “not achieved” in the following categories:\n",
    "\n",
    "1. **brand coherence**\n",
    "2. **target design**\n",
    "3. **visual aesthetics**\n",
    "\n",
    "---\n",
    "\n",
    "**Find the evaluated image-grids under:**\n",
    "\n",
    "[Anhang 03_dreambooth x controlnet](anhang/anhang%2003_dreambooth%20x%20controlnet/)\n",
    "\n",
    "[Anhang_05_nesquik dreamboot hx controlnet](anhang/anhang%2005_nesquik_dreambooth%20x%20controlnet/)\n",
    "\n",
    "**and the ControlNet mask under:**\n",
    "\n",
    "[ControlNet Mask](assets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996db37-5f36-47c1-af45-7d9ef003efe5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc80976-5abf-40b4-80c6-696a246b73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/cluster/user/ehoemmen/.cache\n",
    "%env HF_DATASETS_CACHE=/cluster/user/ehoemmen/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ae5ed-78ea-4355-8880-61f33aef64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers --upgrade -q\n",
    "!pip install opencv-python transformers mediapipe matplotlib accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe178ae-93ca-4903-9140-ae6b2a59d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9e0b5-e9e1-439e-ba98-2e40b83c20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid\n",
    "from PIL import Image\n",
    "\n",
    "#Image Grid\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b6054-5650-4675-a13f-02ec4cd7b262",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12694cd8-3421-4dc7-b2f3-37a573f81dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models and pipeline\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    ")\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",  controlnet=controlnet, vae=vae, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    ")\n",
    "\n",
    "#load fine-tuned lora-weights\n",
    "pipe.load_lora_weights(\"erikhsos/nesquik_15-images_LoRA_lr1-4_2000\")\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12e144-c27b-4b9e-b0c9-40edc9187d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unload lora weights\n",
    "\n",
    "pipe.unload_lora_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522d680-a214-405c-aae3-b01ab0e9d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "\n",
    "# list of colors\n",
    "colors = [\"light green\", \"light blue\", \"olive\", \"grey\"]\n",
    "\n",
    "# prompt for different colors\n",
    "prompts = [f\"a [CB] bottle photo with a {color} label with the text CAMPUSBIER\" for color in colors]\n",
    "neg_prompt=\"green label, brown bottle\"\n",
    "\n",
    "# load original image\n",
    "image = load_image(\n",
    "   '/cluster/user/ehoemmen/development/tests_sonstiges/05_Masterarbeit/03_inpainting/campusbier_input.png' #enter path\n",
    ")\n",
    "\n",
    "# create canny edge image\n",
    "image = np.array(original_image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "# use specific seed, if necessary\n",
    "generator = torch.manual_seed(493)\n",
    "\n",
    "# create images\n",
    "generated_images = pipe(\n",
    "    prompts,\n",
    "    negative_prompt=neg_prompt,\n",
    "    num_inference_steps=25,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale, \n",
    "    image=canny_image,\n",
    "    generator=generator\n",
    ").images\n",
    "\n",
    "# create image grid\n",
    "grid = image_grid(generated_images, rows=1, cols=num_images)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b15184-dbe4-4887-88f3-ec2753503713",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82736566-ee58-42d3-b5af-08d1f53b6efb",
   "metadata": {},
   "source": [
    "The results could be **improved in all evaluated categories** so that **more relevant designs** can be generated overall.\n",
    "\n",
    " Now it is **possible to generate text** by specifying it via a manually created sketch.\n",
    "\n",
    " To futher improve the designs and change it regionally please check out [**Chapter 5.3. Inpainting**](5.3.%20inpainting.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
