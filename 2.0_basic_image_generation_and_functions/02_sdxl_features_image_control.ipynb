{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094f9761-3c95-4da0-a71c-eaa8a090aeae",
   "metadata": {},
   "source": [
    "# 02. Basic Image Generation + Functions\n",
    "\n",
    "## 02. SDXL Features + Image Control\n",
    "\n",
    "#### Content:\n",
    "0.  [SDXL - General](#sdxlgeneral)\n",
    "1.  [Basic Image Generation](#basicgeneration)\n",
    "2.  [Guidance Scale](#guidance)\n",
    "3.  [Prompt Weighting](#weighting)\n",
    "4.  [Image Grid](#grid)\n",
    "5.  [Deterministic Generation](#deterministic)\n",
    "6.  [Image Diffusion Process Grid](#processgrid)\n",
    "7.  [Key-Findings](#keyfind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca3494-8402-4d3d-a20c-def6ae9eecfc",
   "metadata": {},
   "source": [
    "## Description + Links\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cd76c-d673-4235-adc2-d4beca9f9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/cluster/user/ehoemmen/.cache\n",
    "%env HF_DATASETS_CACHE=/cluster/user/ehoemmen/.cache\n",
    "%env TRANSFORMERS_CACHE=/cluster/user/ehoemmen/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25044620-a6b9-46c6-95d5-a6ad881e2a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U diffusers invisible_watermark transformers accelerate safetensors datasets scipy torchsde compel mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c0fe8-5bcc-4b06-9d5f-5c445111cbcf",
   "metadata": {},
   "source": [
    "<a id=\"sdxlgeneral\"></a>\r\n",
    "## SDXL - General Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d175501-86ee-46f7-b92a-d599f3ccf44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# compel for prompt weighting\n",
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "\n",
    "# for image grid\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae936cb3-18b7-435e-8135-7382f73b749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always set chache_dir to your folder to avoid \"out of memory\"-errors\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "  cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    "  variant=\"fp16\",\n",
    "  use_safetensors=True,\n",
    "  torch_dtype=torch.float16,\n",
    ")\n",
    "#.to(\"cuda\")\n",
    "\n",
    "#enable_sequential_cpu_offload() to avoid \"out of memory\"-errors - then don't move the pipe to CUDA\n",
    "pipeline.enable_sequential_cpu_offload()\n",
    "\n",
    "#Prompt Weighting -set up compel\n",
    "compel = Compel(\n",
    "  tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2] ,\n",
    "  text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2],\n",
    "  returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "  requires_pooled=[False, True]\n",
    ")\n",
    "\n",
    "#Image Grid\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25276eb-5110-406f-9e37-3980a305161f",
   "metadata": {},
   "source": [
    "<a id=\"basicgeneration\"></a>\n",
    "\n",
    "### 1. Basic Image Generation\n",
    "\n",
    "SDXL uses **two text encoders** (CLIP ViT-L and OpenCLIP ViT-bigG). You can access them with 'prompt' and 'prompt_2' and also 'negative_prompt' and 'negative_prompt_2'.\n",
    "* 'prompt' is used for the primary description of the image\n",
    "* 'prompt_2' is used for additional information like the style, atmosphere or background details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd06dc-2809-4e0d-be67-6205117109fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=30\n",
    "\n",
    "prompt = \"mockup of an organic milk package with a cow on it, organic aesthetic\"\n",
    "# prompt2 = \"organic aesthetic, green and brown colors\"\n",
    "\n",
    "# neg_prompt = \"red ball\"\n",
    "# neg_prompt2 = None\n",
    "\n",
    "# generate image // set manuel seed \n",
    "generator = torch.Generator().manual_seed(33)\n",
    "image = pipeline(prompt=prompt, \n",
    "                 prompt_2=prompt2,\n",
    "                 # negative_prompt=neg_prompt,\n",
    "                 # negative_prompt_2=neg_prompt2,\n",
    "                 generator=generator, \n",
    "                 height=1024,\n",
    "                 width=1024,\n",
    "                 num_inference_steps=n_steps,\n",
    "                ).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df5dc6-fab0-4114-9d5c-3876f3944425",
   "metadata": {},
   "source": [
    "<a id=\"guidance\"></a>\n",
    "## 2. Guidance Scale\n",
    "Is a scale for **classifier-free guidance (cfg)**.\n",
    "\n",
    "Higher guidance scale  encourages to generate images that are closely linked to the text prompt, usually at the expense of lower image quality. The Default value is **7.5**.\n",
    "Here we're creating an image grid with four different cfg-values, to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a84879-9753-473c-9311-3e37d1ca4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 4\n",
    "prompt = \"a red cat playing with a ball\"\n",
    "\n",
    "cfg_values = [1, 7, 12, 17]\n",
    "\n",
    "images = []\n",
    "for cfg_value in cfg_values:\n",
    "    generated_images = pipeline(prompt=prompt, \n",
    "                                #generator=generator, \n",
    "                                num_inference_steps=20,\n",
    "                                #height=512,width=512,\n",
    "                                num_images_per_prompt=n_images,\n",
    "                                guidance_scale=cfg_value\n",
    "                                ).images[0]\n",
    "    images.append(generated_images)\n",
    "\n",
    "# create grid\n",
    "grid = image_grid(images, rows=1, cols=4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e743eb-bc76-4eac-91b2-ed2aa0059643",
   "metadata": {},
   "source": [
    "<a id=\"weighting\"></a>\n",
    "## 3. Prompt weighting\n",
    "\n",
    "More control by focusing on specific parts of the prompt. It works by increasing or decreasing the scale of the text embedding vector that corresponds to its concept in the prompt because you may not necessarily want the model to focus on all concepts equally. Use **Compel** for prompt weighting.\n",
    "\n",
    "To **increase or decrease the weight** of a word use + or - (+ corresponds to the value 1.1, ++ corresponds to 1.2, and so on. Similarly, - corresponds to 0.9 and -- corresponds to 0.8).\n",
    "\n",
    "Or use the word and the weight. E.g. upweight \"red ball\" by the factor 1.5 and downweight \"cat\" by 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b26674-d20c-4601-a62f-7a29f1638ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 1\n",
    "prompt = \"a cat playing with a (red ball)1.5\"\n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "# generate image\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "image = pipeline(prompt_embeds=conditioning, \n",
    "                 pooled_prompt_embeds=pooled, \n",
    "                 generator=generator, \n",
    "                 num_inference_steps=30,\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85486c-e843-46da-81ec-84287abed875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "prompt = \"a cat playing with a red ball--------\"\n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "# generate image\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "image = pipeline(prompt_embeds=conditioning, \n",
    "                 pooled_prompt_embeds=pooled, \n",
    "                 generator=generator, \n",
    "                 num_inference_steps=30,\n",
    "                ).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb6bae-71c1-431e-9f59-20244a20a658",
   "metadata": {},
   "source": [
    "<a id=\"grid\"></a>\n",
    "## 4. Image Grid\n",
    "\n",
    "Creating an image grid makes is much easier to handle the generation and evaluate results, compared to a single image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73728b8f-4713-4b29-9675-5a3be2a7fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mehrere Bilder mit gleichem Prompt als Grid erstellen (hier 3)\n",
    "\n",
    "num_images = 3\n",
    "prompt = [\"a photograph of an astronaut riding a horse\"] * num_images\n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "images = pipeline(prompt_embeds=conditioning, \n",
    "                  pooled_prompt_embeds=pooled, \n",
    "                  num_inference_steps=30,\n",
    "                 ).images\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=3)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a270d-4d8f-41d8-a59b-970e34725bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehrere Bilder mit gleichem Prompt als Grid + Prompt Weighting\n",
    "\n",
    "num_images = 4\n",
    "prompt = [\"a photograph of an (astronaut)0.5 riding a (horse)1.5\"] * num_images\n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "images = pipeline(prompt_embeds=conditioning, \n",
    "                  pooled_prompt_embeds=pooled, \n",
    "                  num_inference_steps=30,\n",
    "                 ).images\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffeb4d-2941-4594-a511-56c45e5ded1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mehrere Bilder mit gleichem Prompt als Grid\n",
    "#jedes +/- = weighting von 0,1\n",
    "\n",
    "num_images = 4\n",
    "prompt = [\"a photograph of an astronaut----- riding a horse+++++\"] * num_images\n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "images = pipeline(prompt_embeds=conditioning, \n",
    "                  pooled_prompt_embeds=pooled, \n",
    "                  num_inference_steps=30,\n",
    "                 ).images\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=4)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89827885-7626-40fa-ab71-a3500e7d0840",
   "metadata": {},
   "source": [
    "<a id=\"deterministic\"></a>\n",
    "## 5. Deterministic generation\n",
    "\n",
    "You can test and improve the image quality by iterating of different specifications like a more detailed prompt or play with the seed. The **batch generation** makes it super easy compare results and see what impact the changes have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d59fcd-46bd-43fc-b93a-ebfe50011049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 4 images with different seed\n",
    "\n",
    "n_images = 4\n",
    "prompt = [\"a red cat playing with a ball\"] \n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "generator = [torch.Generator(device=\"cuda\").manual_seed(i) for i in range(n_images)]\n",
    "\n",
    "images = pipeline(prompt_embeds=conditioning, \n",
    "                 pooled_prompt_embeds=pooled, \n",
    "                 generator=generator, \n",
    "                 num_inference_steps=30,\n",
    "                 num_images_per_prompt=n_images,\n",
    "                ).images\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfaa327-3c41-412c-a24e-10868cb016e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve one image with 4 different additional prompts\n",
    "\n",
    "n_images = 4\n",
    "prompt = [\"a red cat playing with a ball\" + t for t in [\", highly realistic\", \", artsy\", \", trending\", \", colorful\"]]\n",
    "\n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "# generate image\n",
    "generator = [torch.Generator().manual_seed(10) for i in range(n_images)]\n",
    "\n",
    "images = pipeline(prompt_embeds=conditioning, \n",
    "                 pooled_prompt_embeds=pooled, \n",
    "                 generator=generator, \n",
    "                 num_inference_steps=30,\n",
    "                ).images\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f86a82-a28a-4746-8357-172249c2d681",
   "metadata": {},
   "source": [
    "<a id=\"processgrid\"></a>\n",
    "## 6. Image Diffusion Process Grid\n",
    "The illustrated diffusion process. You can see the generation starts with random noise and removes it over time until reaching a high quality image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf0bef-cbc3-4458-b875-7227625558e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eine Bildreihe mit einem Prompt\n",
    "\n",
    "n_images = 5\n",
    "prompt = [\"a red cat playing with a ball\"] \n",
    "conditioning, pooled = compel(prompt)\n",
    "\n",
    "# Create a list of denoising_end values\n",
    "end = 1.0 / n_images\n",
    "denoising_ends = [end * (step + 1) for step in range(n_images)]\n",
    "\n",
    "# Seed setzen\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(2147483647) \n",
    "\n",
    "# Initialize images list\n",
    "images = []\n",
    "\n",
    "# Generate images for each denoising_end value\n",
    "for i in range(n_images):\n",
    "    generator.manual_seed(2147483647)\n",
    "    image = pipeline(prompt_embeds=conditioning, \n",
    "                     pooled_prompt_embeds=pooled, \n",
    "                     generator=generator, \n",
    "                     num_inference_steps=30,\n",
    "                     denoising_end=denoising_ends[i]\n",
    "                    ).images[0]\n",
    "    images.append(image)\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=n_images)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445a1d3-0506-420e-ac6a-da6ffbd96bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mehrere Bildreihen mit mehreren Prompts\n",
    "\n",
    "n_images = 5\n",
    "prompts = [\n",
    "    \"a red cat playing with a ball\",\n",
    "    \"a blue dog chasing its tail\",\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# Seed setzen\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(2147483647)\n",
    "\n",
    "# Create a list of denoising_end values\n",
    "end = 1.0 / n_images\n",
    "denoising_ends = [end * (step + 1) for step in range(n_images)]\n",
    "\n",
    "# Liste, die die Bildgenerierungen speichert\n",
    "all_images = [] \n",
    "\n",
    "for prompt in prompts:\n",
    "    conditioning, pooled = compel([prompt]) \n",
    "    images = []\n",
    "\n",
    "    # Generate images for each denoising_end value\n",
    "    # hier muss nochmal der manuelle seed gesetzt werden \n",
    "    for i in range(n_images):\n",
    "        generator.manual_seed(2147483647)\n",
    "        image = pipeline(prompt_embeds=conditioning, \n",
    "                         pooled_prompt_embeds=pooled, \n",
    "                         generator=generator, \n",
    "                         num_inference_steps=30,\n",
    "                         denoising_end=denoising_ends[i]\n",
    "                        ).images[0]\n",
    "        images.append(image)\n",
    "    \n",
    "    all_images.extend(images)\n",
    "\n",
    "grid = image_grid(all_images, rows=len(prompts), cols=n_images)\n",
    "\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a017593-3fad-45e3-814c-ca969ae74b19",
   "metadata": {},
   "source": [
    "<a id=\"keyfind\"></a>\n",
    "## 7. Key Findings\n",
    "\n",
    "#### Prompt_2\n",
    "\n",
    "No matter what I tested with the **second text encoder** (`prompt_2`), the result was always **worse** when this was used. Actually, secondary information about the background or the style of the image should be specified here. That's why I then **only used the simple 'prompt'** and did not specify any further information via 'prompt_2' and left it empty.\n",
    "* All information regarding style and coloring etc. should therefore always be appended to the normal **`prompt` separated by commas**, e.g:\n",
    "  \n",
    "  `\"mockup of an organic milk package with a cow and the text 'Organic', organic aesthetic, natural green and brown colors\"`\n",
    "\n",
    "#### Guidance Scale\n",
    "\n",
    "It has proven useful to always create images based on the default value **7.5**. If the results do not meet your expectations, you have a variable with the cfg parameter that is worth readjusting. This has a big effect on the generated image.\n",
    "\n",
    "#### Prompt Weighting\n",
    "\n",
    "\n",
    "There are two ways of writing prompt weighting: (word)1.5 or (word)+++++. The decimal notation is more suitable, as it also allows word combinations to be weighted and the prompt is not unnecessarily long due to the many + or - characters. for example\n",
    "\n",
    "`\"cat playing with a (red ball)1.6\"`\n",
    "\n",
    "Sometimes it is necessary to play around with the weighting (which word(s) (combination) + how high the weighting) in order to achieve the desired result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b24039-7088-4a29-b785-7d473b4f10f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
