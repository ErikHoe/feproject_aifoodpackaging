{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b93190-3e7b-4adf-9cdb-2b2132d603bb",
   "metadata": {},
   "source": [
    "# 02. Basic Image Generation + Functions\n",
    "\n",
    "## 01. Basic SDXL Pipeline\n",
    "#### Content\n",
    "\n",
    "1.  [Load SDXL Pipeline](#basicsdxl)\n",
    "2.  [SDXL Architecture Components](#sdxlarchitecture)\n",
    "3.  [SDXL Base Model](#sdxlbase)\n",
    "4.  [SDXL Base + Refiner](#sdxlbaserefiner)\n",
    "\n",
    "---\n",
    "\n",
    "**Reduce memory usage** (avoid out-of-memory-errors) see: \n",
    "https://huggingface.co/docs/diffusers/optimization/memory\n",
    "\n",
    "---\n",
    "## Description + Links\n",
    "\n",
    "If you want to know more about the Stable Diffusion Pipline check out this notebook [<u>SDXL - Explained</u>](../1.0_general/02_definitions.ipynb).\n",
    "\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl\n",
    "\n",
    "**Paper**\n",
    "\n",
    "[Podell, D., et al. (2023): SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/abs/2307.01952)\n",
    "\n",
    "\n",
    "[Rombach, R., et al. (2021): High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341d2eb-50d1-45e7-8f69-894d2a535dd9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c755c-16e8-4c71-b859-7d29772c0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/cluster/user/ehoemmen/.cache\n",
    "%env HF_DATASETS_CACHE=/cluster/user/ehoemmen/.cache\n",
    "%env TRANSFORMERS_CACHE=/cluster/user/ehoemmen/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3526d0-1d74-445b-887e-be6e319aa33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U diffusers invisible_watermark transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb30c9-cf57-49f4-8a0d-d8a61d2c40e6",
   "metadata": {},
   "source": [
    "<a id=\"basicsdxl\"></a>\n",
    "\n",
    "## 01. Load SDXL Pipeline\n",
    "\n",
    "The Stable Diffusion XL Pipline consists of two stages - the **base model** and the **refiner model**. The base model could also be run as a standalone model and the expert during the high-noise diffusion stage. The refiner is expert during the low-noise diffusion stage and adds high-quality details. The generated image from the base model could be passed to the refiner to add more details. Using both the base and refiner model together to generate an image, is known as an **ensemble of expert denoisers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531fcda-7bd7-45cf-9412-96ea6c31fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both the base and refiner model\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "    torch_dtype=torch.float16, \n",
    "    variant=\"fp16\", \n",
    "    cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "#normally it's \"base.to(\"cuda\")\" - but to avoid \"out of memory-errors\" we use the enable_sequential_cpu_offload() or enable_model_cpu_offload()\n",
    "base.enable_model_cpu_offload()\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "\n",
    "#to avoid \"out of memory-errors\" we use the enable_model_cpu_offload() or enable_sequential_cpu_offload() instead of using CUDA\n",
    "refiner.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3599561-9469-4e07-92bd-3a3a2d57ed80",
   "metadata": {},
   "source": [
    "<a id=\"sdxlarchitecture\"></a>\n",
    "\n",
    "## 02. SDXL Architecture Components\n",
    "\n",
    "To learn more about the SDXL architecture check out the [SDXL Architecture - Explained](../1.0_General/02_Definitions.ipynb) for defintions or the [Hugging Face - Diffusion Models Class (DM from Scratch) ](../HF_Diffusion%20Models%20Class) to learn Diffusion Models from scratch.\n",
    "\n",
    "You can access any part of the pipline by typing `base.unet`, `base.scheduler` or go even deeper with `base.unet.parameters`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e9fdf-3eec-462a-b263-5e80904a1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipline Components\n",
    "print(list(base.components.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d4417-7eb8-4648-9067-dba2f94c8b60",
   "metadata": {},
   "source": [
    "#### UNet - Number of Parameters\n",
    "\n",
    "Access the UNet and show the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb8ad1-1f36-4b0b-bf1c-b9fb2ad9eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.unet.num_parameters(only_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c834f8-1487-474d-b537-602be17eee19",
   "metadata": {},
   "source": [
    "#### The Tokenizer and Text Encoder\n",
    "\n",
    "text encoder is to **turn an input string (the prompt) into a numerical representation that can be fed to the UNet as conditioning**. The text is first turned into a series of tokens using the pipeline's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07c7be-f04a-4502-8b4c-aaed92f56719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and encoding an example prompt manually\n",
    "\n",
    "# Tokenize\n",
    "input_ids = base.tokenizer([\"A painting of a flooble\"])['input_ids']\n",
    "print(\"Input ID -> decoded token\")\n",
    "for input_id in input_ids[0]:\n",
    "  print(f\"{input_id} -> {base.tokenizer.decode(input_id)}\")\n",
    "\n",
    "# Feed through CLIP text encoder\n",
    "input_ids = torch.tensor(input_ids).to()\n",
    "with torch.no_grad():\n",
    "  text_embeddings = base.text_encoder(input_ids)['last_hidden_state']\n",
    "print(\"Text embeddings shape:\", text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e261336-238b-4ee0-b0c0-a85cafb665de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.text_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e19da-9bfb-4cab-a437-bb562e5930a0",
   "metadata": {},
   "source": [
    "<a id=\"sdxlbase\"></a>\n",
    "## 03. SDXL Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de8699-36b5-450a-8043-9f444212bb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base model image generation \n",
    "prompt = \"delicious risotto in a pan, food photography, realistic, top view, professional lightning\"\n",
    "\n",
    "# all parameters are used in default here\n",
    "images = base(prompt=prompt,\n",
    "              variant=\"fp16\",\n",
    "              torch_dtype=torch.float16,\n",
    "             ).images[0]\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff45e2-281e-441c-ac51-bb7fe6b0a2a8",
   "metadata": {},
   "source": [
    "### SDXL Parameters\n",
    "Key arguments to tweak in the pipeline:\n",
    "\n",
    "* **width** and **height** specify the size of the generated image. They must be **divisible by 8 for the VAE** to work\n",
    "* the **number of steps** influences the generation quality. The default (50) works well but in some cases you can get away with as few as 20 steps which is handy for experimentation\n",
    "* the **negative prompt** is used during the classifier-free guidance process, and can be a useful way to add additional control.\n",
    "* the `guidance_scale` argument determines how strong the **classifier-free guidance (CFG)** is. Higher scales push the generated images to **better match the prompt**, but if the scale is too hig hthe results can become over-saturated and unpleasant\n",
    "\n",
    "Here you can find all the different [**SDXL Parameters**](../1.0_general/03_parameters.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22ba18-acfd-4aa5-9c43-2f8de7894296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model image generation \n",
    "prompt = \"delicious risotto in a pan, food photography, realistic, top view, professional lightning\"\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "\n",
    "images = base(\n",
    "        prompt=prompt,             # What to generate\n",
    "        negative_prompt=\"oversaturated, blurry, low quality\", # What NOT to generate\n",
    "        height=1024, width=1024,   # Specify the image size\n",
    "        guidance_scale=8,          # How strongly to follow the prompt\n",
    "        num_inference_steps=35,    # How many steps to take\n",
    "        generator=generator        # Fixed random seed\n",
    "        ).images[0]\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdc6f6-e20f-4219-9ca0-a69b149d974b",
   "metadata": {},
   "source": [
    "<a id=\"sdxlbaserefiner\"></a>\n",
    "## 04. SDXL Base + Refiner\n",
    "\n",
    "To use this approach, you need to define the number of timesteps for each model to run through their respective stages. For the base model, this is controlled by the `denoising_end` parameter and for the refiner model, it is controlled by the `denoising_start` parameter. These parameters should be a float between 0 and 1. \n",
    "\n",
    "Let’s set `denoising_end=0.8` so the base model performs the first 80% of denoising the high-noise timesteps and set `denoising_start=0.8` so the refiner model performs the last 20% of denoising the low-noise timesteps. The base model output should be in **latent space** instead of a PIL image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90320b17-d99c-4818-bcfd-279934b3959f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here the generated image from the base model is passed to the refiner pipline after 80% of the denoising steps\n",
    "\n",
    "prompt = \"delicious risotto in a pan, food photography, realistic, top view, professional lightning\"\n",
    "\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=40,\n",
    "    denoising_end=0.8,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=40,\n",
    "    denoising_start=0.8,\n",
    "    image=image,\n",
    ").images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
