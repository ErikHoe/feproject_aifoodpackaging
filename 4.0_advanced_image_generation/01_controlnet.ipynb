{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5b109e-b0cb-4dc3-bb03-633dc1aad6a3",
   "metadata": {},
   "source": [
    "# 04. Advanced Image Generation\n",
    "\n",
    "## 01. ControlNET\n",
    "\n",
    "Specific tests for the ControlNETs can be found directly in the respective sections below.\n",
    "\n",
    "#### Content\n",
    "\n",
    "1. [Canny Edge](#cannyedge)\n",
    "2. [Open Pose](#openpose)\n",
    "3. [Depth](#depth)\n",
    "4. [Scribble](#scribble)\n",
    "5. [M-LSD Line](#mlsdline)\n",
    "6. [HED Boundary Vision](#hedboundary)\n",
    "7. [Image Segmentation](#segmentation)\n",
    "8. [Normal Map](#normalmap)\n",
    "\n",
    "--- \n",
    "\n",
    "09. [Dreambooth x ControlNet](#dreamboothcontrolnet)\n",
    "10. [Combining Multible Conditionings](#combining)\n",
    "11. [Key-Findings](#keyfindings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16effe6-0e5e-493e-a770-cc88d3898131",
   "metadata": {},
   "source": [
    "## Description + Links\n",
    "\n",
    "* controlling image diffusion models by conditioning the model with an additional input image\n",
    "* there are several ways of conditioning (canny edge, user sketching, human pose, depth, and more)\n",
    "\n",
    "If you want to know more about ControlNET check out this [<u>Definitions Notebook</u>](../1.0_general/02_definitions.ipynb) under point 06. ControlNET.\n",
    "\n",
    "---\n",
    "**Documentation**\n",
    "\n",
    "https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl\n",
    "\n",
    "https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlnet#controlnet\n",
    "\n",
    "https://huggingface.co/blog/controlnet\n",
    "\n",
    "**Paper**\n",
    "\n",
    "[Zhang, L., et al. (2023): Adding conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e351d8-5d1d-4e41-b1a4-e02244e27629",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007658f-7bbf-4dfc-8ab8-07dc36e56062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/cluster/user/ehoemmen/.cache\n",
    "%env HF_DATASETS_CACHE=/cluster/user/ehoemmen/.cache\n",
    "%env TRANSFORMERS_CACHE=/cluster/user/ehoemmen/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5bad1-de2f-4f52-8f39-c82cd99d6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q -U diffusers controlnet_aux transformers accelerate mediapipe matplotlib opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dea9f2-ac7f-41ec-95d6-4b47fe255915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "from controlnet_aux import OpenposeDetector, MLSDdetector\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063bd8d-0b64-4d4f-9832-9922dfc3cdc9",
   "metadata": {},
   "source": [
    "<a id=\"cannyedge\"></a>\r\n",
    "## 01. Canny Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cd2f3-66a1-4a99-adcf-80c93c0738b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models and pipeline\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",  controlnet=controlnet, vae=vae, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "#pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ddf0e-bb5f-43f1-840b-802bfa7555fc",
   "metadata": {},
   "source": [
    "#### Some General Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69a936-48e9-409c-a6ff-714536f7e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"happy, red cat in the jungle\"\n",
    "#negative_prompt = \"low quality, bad quality, sketches\"\n",
    "\n",
    "#download an image\n",
    "original_image = load_image(\n",
    "'../5.0_pictures/majestic_lion.png'\n",
    ")\n",
    "\n",
    "# get canny image\n",
    "image = np.array(original_image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "# generate image\n",
    "generated_image = pipe(\n",
    "    prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image\n",
    ").images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(canny_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Canny Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(generated_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191840b-11ab-4199-97c2-d807e0697165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"elephant\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "n_steps=50\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "# download an image\n",
    "image = load_image(\n",
    "'../5.0_pictures/majestic_lion.png'\n",
    ")\n",
    "\n",
    "# generate image\n",
    "generated_image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt = negative_prompt,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale, \n",
    "    num_inference_steps=n_steps,\n",
    "    image=canny_image,\n",
    "    #guess_mode=True,\n",
    ").images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(canny_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Canny Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(generated_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bff82f-337d-4188-bd96-af585f332e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Das ursprüngliche Bild laden\n",
    "original_image = load_image(../5.0_pictures/majestic_lion.png')\n",
    "\n",
    "# Das generierte Bild erstellen\n",
    "generated_image = pipe(\n",
    "    prompt, \n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale, \n",
    "    image=canny_image\n",
    ").images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(canny_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Canny Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(generated_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ec2d7-3f50-45ec-8bd1-763a5cce952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das ursprüngliche Bild laden\n",
    "original_image = load_image('../5.0_pictures/majestic_lion.png')\n",
    "\n",
    "# Das generierte Bild erstellen\n",
    "generated_image = pipe(\n",
    "    prompt, \n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale, \n",
    "    image=canny_image\n",
    ").images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(canny_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Canny Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(generated_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c507b0-e2f5-460d-8a28-133edd2e5be1",
   "metadata": {},
   "source": [
    "#### Canny Edge - Kellogg's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff5832-4acf-4b0e-8c57-e9cafc370c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Honey Bites Cornflakes, with Bees\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "n_steps=50\n",
    "controlnet_conditioning_scale = 0.7  # recommended for good generalization\n",
    "\n",
    "# download an image\n",
    "image = load_image(\n",
    "'../5.0_pictures/kellogsfrosties_removebg_preview.jpg'\n",
    ")\n",
    "\n",
    "# download an image\n",
    "original_image = load_image(\n",
    "'../5.0_pictures/kellogsfrosties_removebg_preview.jpg'\n",
    ")\n",
    "\n",
    "# get canny image\n",
    "image = np.array(image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "# generate image\n",
    "generated_image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt = negative_prompt,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale, \n",
    "    num_inference_steps=n_steps,\n",
    "    image=canny_image\n",
    ").images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(canny_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Canny Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(generated_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb8b19-d751-4eb3-8547-2f3e9da2df1e",
   "metadata": {},
   "source": [
    "#### Canny Edge - Kellogg's Test \n",
    "\n",
    "Without a Prompt --> \"Guess Mode\"\n",
    "\n",
    "In the Guess Mode the model predicts the output, without further inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5ccf1-ac51-4894-9a33-7d211b9a206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "n_steps=50\n",
    "controlnet_conditioning_scale = 0.7  # recommended for good generalization\n",
    "\n",
    "\n",
    "# download an image\n",
    "image = load_image(\n",
    "'../5.0_pictures/kellogsfrosties-simple.jpg'\n",
    ")\n",
    "\n",
    "# download an image\n",
    "original_image = load_image(\n",
    "'../5.0_pictures/kellogsfrosties-simple.jpg'\n",
    ")\n",
    "cluster/upload/5.0_pictures/kellogsfrosties-simple.jpg\n",
    "# get canny image\n",
    "image = np.array(image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "# generate image\n",
    "generated_image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt = negative_prompt,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale, \n",
    "    num_inference_steps=n_steps,\n",
    "    image=canny_image\n",
    ").images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(canny_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Canny Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(generated_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280a2b7-bb11-42fd-bf9f-13578c47a2ba",
   "metadata": {},
   "source": [
    "<a id=\"openpose\"></a>\r\n",
    "## 2. Open Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87eafd-a7d2-4cac-a0a8-78dfc0c28c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute openpose conditioning image\n",
    "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\",cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n",
    ")\n",
    "openpose_image = openpose(image)\n",
    "\n",
    "# Initialize ControlNet pipeline\n",
    "controlnet = ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16,cache_dir=\"/cluster/user/ehoemmen/.cache\",)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\",\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33606243-2d6c-4747-9274-8d75c04b8c00",
   "metadata": {},
   "source": [
    "#### General Open Pose Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead6853-191f-4f3d-a99f-1c78792588fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Generation\n",
    "prompt = \"Elon Musk in a desert, high quality\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "images = pipe(\n",
    "    prompt, \n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=40,\n",
    "    image=openpose_image.resize((1024, 1024)),\n",
    "    generator=torch.manual_seed(97),\n",
    ").images\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(openpose_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Openpose Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images[0])\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ab05e-5055-45c3-8eb8-57f95770d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\n",
    "'../5.0_pictures/pngwing.com.png'\n",
    ")\n",
    "openpose_image = openpose(image)\n",
    "\n",
    "# Image Generation\n",
    "prompt = \"Happy Cat\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "images = pipe(\n",
    "    prompt, \n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=40,\n",
    "    image=openpose_image.resize((1024, 1024)),\n",
    "    generator=torch.manual_seed(97),\n",
    ").images\n",
    "images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(openpose_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Openpose Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images[0])\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b672b-b552-4294-bd15-683c691b334c",
   "metadata": {},
   "source": [
    "<a id=\"depth\"></a>\r\n",
    "## 3. Depth\n",
    "https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d09fd0-e632-4ba6-a010-5e89c002fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-depth-sdxl-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ").to(\"cuda\")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16,cache_dir=\"/cluster/user/ehoemmen/.cache\").to(\"cuda\")\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    vae=vae,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d465e-50a1-4080-915a-a3b06a8af280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_map(image):\n",
    "    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        depth_map = depth_estimator(image).predicted_depth\n",
    "\n",
    "    depth_map = torch.nn.functional.interpolate(\n",
    "        depth_map.unsqueeze(1),\n",
    "        size=(1024, 1024),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "    image = torch.cat([depth_map] * 3, dim=1)\n",
    "\n",
    "    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "prompt = \"stormtrooper lecture, photorealistic\"\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "depth_image = get_depth_map(image)\n",
    "\n",
    "images = pipe(\n",
    "    prompt, \n",
    "    image=depth_image, \n",
    "    num_inference_steps=30, \n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale\n",
    ").images\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(depth_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Depth Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images[0])\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title(prompt)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898c16c-15df-4457-bc05-b8c6bf2d2d42",
   "metadata": {},
   "source": [
    "#### Depth - Kellogg's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856954c-3b95-4abb-8480-6e08e9b8ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cereals generation\n",
    "def get_depth_map(image):\n",
    "    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        depth_map = depth_estimator(image).predicted_depth\n",
    "\n",
    "    depth_map = torch.nn.functional.interpolate(\n",
    "        depth_map.unsqueeze(1),\n",
    "        size=(1024, 1024),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "    image = torch.cat([depth_map] * 3, dim=1)\n",
    "\n",
    "    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "prompt = \"organic cereals for kids\"\n",
    "image = load_image('../5.0_pictures/kellogsfrosties_removebg_preview.jpg')\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "depth_image = get_depth_map(image)\n",
    "\n",
    "images = pipe(\n",
    "    prompt, \n",
    "    image=depth_image, \n",
    "    num_inference_steps=30, \n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale\n",
    ").images\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(depth_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Depth Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images[0])\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title(prompt)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786af196-3ae2-4deb-a26b-f23fc8837ce1",
   "metadata": {},
   "source": [
    "<a id=\"scribble\"></a>\r\n",
    "## 4. (Fake) Scribble\n",
    "Fake Scribble -> If you don't want to draw your own scribbles. The script use the same scribble-based model but use a simple algorithm to synthesize scribbles from input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9d6ad-e1b3-44f3-bc55-45d7080565c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from controlnet_aux import HEDdetector\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "hed = HEDdetector.from_pretrained('lllyasviel/Annotators', cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "oimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png\")\n",
    "\n",
    "image = hed(oimage, scribble=True)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-scribble\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"bag\", image, num_inference_steps=20).images[0]\n",
    "\n",
    "# image.save('images/bag_scribble_out.png')\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(oimage)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Sketch')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title(prompt)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990cb03c-c50e-42fb-bf19-e2db96086b45",
   "metadata": {},
   "source": [
    "#### (Fake Scribble) - Kellogg's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f12f94-5a54-49d4-a2e7-908f8288b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hed = HEDdetector.from_pretrained('lllyasviel/Annotators', cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "oimage = load_image('../5.0_pictures/kellogsfrosties_removebg_preview.jpg')\n",
    "\n",
    "image = hed(oimage, scribble=True)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-scribble\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"cornflakes chocolate flavour\", image, num_inference_steps=20).images[0]\n",
    "\n",
    "# image.save('images/bag_scribble_out.png')\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(oimage)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Sketch')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title(prompt)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730806fc-9954-47b7-b704-b13a4ccac0a8",
   "metadata": {},
   "source": [
    "<a id=\"mlsdline\"></a>\r\n",
    "## 5. M - LSD Line\n",
    "A monochrome image composed only of white straight lines on a black background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47620896-d3e4-407b-97e6-484d816f1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from controlnet_aux import MLSDdetector\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet', cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png\")\n",
    "\n",
    "newimage = mlsd(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-mlsd\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"children's room\", \n",
    "              newimage, \n",
    "              num_inference_steps=20\n",
    "             ).images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(newimage)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Depth Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e7bf7-22e5-4294-a02c-286ae8ac0fc7",
   "metadata": {},
   "source": [
    "<a id=\"hedboundary\"></a>\r\n",
    "## 6. HED Boundary Version\n",
    "A monochrome image with white soft edges on a black background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a64cc3-e372-4693-b57a-223258e938aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from controlnet_aux import HEDdetector\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "hed = HEDdetector.from_pretrained('lllyasviel/Annotators', cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png\")\n",
    "\n",
    "newimage = hed(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-hed\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"oil painting of handsome old man, masterpiece\", newimage, num_inference_steps=20).images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(newimage)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('HED Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605af80b-4148-4c87-a1d5-32ac346dc9c7",
   "metadata": {},
   "source": [
    "<a id=\"segmentation\"></a>\r\n",
    "## 7. Image Segmentation - ADE20K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46b25f-bb2e-453c-adc5-29c490d3a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a Color Palette\n",
    "\n",
    "palette = np.asarray([\n",
    "    [0, 0, 0],\n",
    "    [120, 120, 120],\n",
    "    [180, 120, 120],\n",
    "    [6, 230, 230],\n",
    "    [80, 50, 50],\n",
    "    [4, 200, 3],\n",
    "    [120, 120, 80],\n",
    "    [140, 140, 140],\n",
    "    [204, 5, 255],\n",
    "    [230, 230, 230],\n",
    "    [4, 250, 7],\n",
    "    [224, 5, 255],\n",
    "    [235, 255, 7],\n",
    "    [150, 5, 61],\n",
    "    [120, 120, 70],\n",
    "    [8, 255, 51],\n",
    "    [255, 6, 82],\n",
    "    [143, 255, 140],\n",
    "    [204, 255, 4],\n",
    "    [255, 51, 7],\n",
    "    [204, 70, 3],\n",
    "    [0, 102, 200],\n",
    "    [61, 230, 250],\n",
    "    [255, 6, 51],\n",
    "    [11, 102, 255],\n",
    "    [255, 7, 71],\n",
    "    [255, 9, 224],\n",
    "    [9, 7, 230],\n",
    "    [220, 220, 220],\n",
    "    [255, 9, 92],\n",
    "    [112, 9, 255],\n",
    "    [8, 255, 214],\n",
    "    [7, 255, 224],\n",
    "    [255, 184, 6],\n",
    "    [10, 255, 71],\n",
    "    [255, 41, 10],\n",
    "    [7, 255, 255],\n",
    "    [224, 255, 8],\n",
    "    [102, 8, 255],\n",
    "    [255, 61, 6],\n",
    "    [255, 194, 7],\n",
    "    [255, 122, 8],\n",
    "    [0, 255, 20],\n",
    "    [255, 8, 41],\n",
    "    [255, 5, 153],\n",
    "    [6, 51, 255],\n",
    "    [235, 12, 255],\n",
    "    [160, 150, 20],\n",
    "    [0, 163, 255],\n",
    "    [140, 140, 140],\n",
    "    [250, 10, 15],\n",
    "    [20, 255, 0],\n",
    "    [31, 255, 0],\n",
    "    [255, 31, 0],\n",
    "    [255, 224, 0],\n",
    "    [153, 255, 0],\n",
    "    [0, 0, 255],\n",
    "    [255, 71, 0],\n",
    "    [0, 235, 255],\n",
    "    [0, 173, 255],\n",
    "    [31, 0, 255],\n",
    "    [11, 200, 200],\n",
    "    [255, 82, 0],\n",
    "    [0, 255, 245],\n",
    "    [0, 61, 255],\n",
    "    [0, 255, 112],\n",
    "    [0, 255, 133],\n",
    "    [255, 0, 0],\n",
    "    [255, 163, 0],\n",
    "    [255, 102, 0],\n",
    "    [194, 255, 0],\n",
    "    [0, 143, 255],\n",
    "    [51, 255, 0],\n",
    "    [0, 82, 255],\n",
    "    [0, 255, 41],\n",
    "    [0, 255, 173],\n",
    "    [10, 0, 255],\n",
    "    [173, 255, 0],\n",
    "    [0, 255, 153],\n",
    "    [255, 92, 0],\n",
    "    [255, 0, 255],\n",
    "    [255, 0, 245],\n",
    "    [255, 0, 102],\n",
    "    [255, 173, 0],\n",
    "    [255, 0, 20],\n",
    "    [255, 184, 184],\n",
    "    [0, 31, 255],\n",
    "    [0, 255, 61],\n",
    "    [0, 71, 255],\n",
    "    [255, 0, 204],\n",
    "    [0, 255, 194],\n",
    "    [0, 255, 82],\n",
    "    [0, 10, 255],\n",
    "    [0, 112, 255],\n",
    "    [51, 0, 255],\n",
    "    [0, 194, 255],\n",
    "    [0, 122, 255],\n",
    "    [0, 255, 163],\n",
    "    [255, 153, 0],\n",
    "    [0, 255, 10],\n",
    "    [255, 112, 0],\n",
    "    [143, 255, 0],\n",
    "    [82, 0, 255],\n",
    "    [163, 255, 0],\n",
    "    [255, 235, 0],\n",
    "    [8, 184, 170],\n",
    "    [133, 0, 255],\n",
    "    [0, 255, 92],\n",
    "    [184, 0, 255],\n",
    "    [255, 0, 31],\n",
    "    [0, 184, 255],\n",
    "    [0, 214, 255],\n",
    "    [255, 0, 112],\n",
    "    [92, 255, 0],\n",
    "    [0, 224, 255],\n",
    "    [112, 224, 255],\n",
    "    [70, 184, 160],\n",
    "    [163, 0, 255],\n",
    "    [153, 0, 255],\n",
    "    [71, 255, 0],\n",
    "    [255, 0, 163],\n",
    "    [255, 204, 0],\n",
    "    [255, 0, 143],\n",
    "    [0, 255, 235],\n",
    "    [133, 255, 0],\n",
    "    [255, 0, 235],\n",
    "    [245, 0, 255],\n",
    "    [255, 0, 122],\n",
    "    [255, 245, 0],\n",
    "    [10, 190, 212],\n",
    "    [214, 255, 0],\n",
    "    [0, 204, 255],\n",
    "    [20, 0, 255],\n",
    "    [255, 255, 0],\n",
    "    [0, 153, 255],\n",
    "    [0, 41, 255],\n",
    "    [0, 255, 204],\n",
    "    [41, 0, 255],\n",
    "    [41, 255, 0],\n",
    "    [173, 0, 255],\n",
    "    [0, 245, 255],\n",
    "    [71, 0, 255],\n",
    "    [122, 0, 255],\n",
    "    [0, 255, 184],\n",
    "    [0, 92, 255],\n",
    "    [184, 255, 0],\n",
    "    [0, 133, 255],\n",
    "    [255, 214, 0],\n",
    "    [25, 194, 194],\n",
    "    [102, 255, 0],\n",
    "    [92, 0, 255],\n",
    "])\n",
    "\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\", cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\", cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-seg/resolve/main/images/house.png\").convert('RGB')\n",
    "\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = image_segmentor(pixel_values)\n",
    "\n",
    "seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "\n",
    "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[seg == label, :] = color\n",
    "\n",
    "color_seg = color_seg.astype(np.uint8)\n",
    "\n",
    "newimage = Image.fromarray(color_seg)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-seg\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"ancient roman house\", image, num_inference_steps=20).images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(newimage)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Segmenation Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4dfc3-4341-4cb1-8701-1d5175ca6693",
   "metadata": {},
   "source": [
    "#### Image Segmentation - Test Kellogg's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7125b-4e73-43a0-9bb4-0760c8cfa350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a Color Palette\n",
    "\n",
    "palette = np.asarray([\n",
    "    [0, 0, 0],\n",
    "    [120, 120, 120],\n",
    "    [180, 120, 120],\n",
    "    [6, 230, 230],\n",
    "    [80, 50, 50],\n",
    "    [4, 200, 3],\n",
    "    [120, 120, 80],\n",
    "    [140, 140, 140],\n",
    "    [204, 5, 255],\n",
    "    [230, 230, 230],\n",
    "    [4, 250, 7],\n",
    "    [224, 5, 255],\n",
    "    [235, 255, 7],\n",
    "    [150, 5, 61],\n",
    "    [120, 120, 70],\n",
    "    [8, 255, 51],\n",
    "    [255, 6, 82],\n",
    "    [143, 255, 140],\n",
    "    [204, 255, 4],\n",
    "    [255, 51, 7],\n",
    "    [204, 70, 3],\n",
    "    [0, 102, 200],\n",
    "    [61, 230, 250],\n",
    "    [255, 6, 51],\n",
    "    [11, 102, 255],\n",
    "    [255, 7, 71],\n",
    "    [255, 9, 224],\n",
    "    [9, 7, 230],\n",
    "    [220, 220, 220],\n",
    "    [255, 9, 92],\n",
    "    [112, 9, 255],\n",
    "    [8, 255, 214],\n",
    "    [7, 255, 224],\n",
    "    [255, 184, 6],\n",
    "    [10, 255, 71],\n",
    "    [255, 41, 10],\n",
    "    [7, 255, 255],\n",
    "    [224, 255, 8],\n",
    "    [102, 8, 255],\n",
    "    [255, 61, 6],\n",
    "    [255, 194, 7],\n",
    "    [255, 122, 8],\n",
    "    [0, 255, 20],\n",
    "    [255, 8, 41],\n",
    "    [255, 5, 153],\n",
    "    [6, 51, 255],\n",
    "    [235, 12, 255],\n",
    "    [160, 150, 20],\n",
    "    [0, 163, 255],\n",
    "    [140, 140, 140],\n",
    "    [250, 10, 15],\n",
    "    [20, 255, 0],\n",
    "    [31, 255, 0],\n",
    "    [255, 31, 0],\n",
    "    [255, 224, 0],\n",
    "    [153, 255, 0],\n",
    "    [0, 0, 255],\n",
    "    [255, 71, 0],\n",
    "    [0, 235, 255],\n",
    "    [0, 173, 255],\n",
    "    [31, 0, 255],\n",
    "    [11, 200, 200],\n",
    "    [255, 82, 0],\n",
    "    [0, 255, 245],\n",
    "    [0, 61, 255],\n",
    "    [0, 255, 112],\n",
    "    [0, 255, 133],\n",
    "    [255, 0, 0],\n",
    "    [255, 163, 0],\n",
    "    [255, 102, 0],\n",
    "    [194, 255, 0],\n",
    "    [0, 143, 255],\n",
    "    [51, 255, 0],\n",
    "    [0, 82, 255],\n",
    "    [0, 255, 41],\n",
    "    [0, 255, 173],\n",
    "    [10, 0, 255],\n",
    "    [173, 255, 0],\n",
    "    [0, 255, 153],\n",
    "    [255, 92, 0],\n",
    "    [255, 0, 255],\n",
    "    [255, 0, 245],\n",
    "    [255, 0, 102],\n",
    "    [255, 173, 0],\n",
    "    [255, 0, 20],\n",
    "    [255, 184, 184],\n",
    "    [0, 31, 255],\n",
    "    [0, 255, 61],\n",
    "    [0, 71, 255],\n",
    "    [255, 0, 204],\n",
    "    [0, 255, 194],\n",
    "    [0, 255, 82],\n",
    "    [0, 10, 255],\n",
    "    [0, 112, 255],\n",
    "    [51, 0, 255],\n",
    "    [0, 194, 255],\n",
    "    [0, 122, 255],\n",
    "    [0, 255, 163],\n",
    "    [255, 153, 0],\n",
    "    [0, 255, 10],\n",
    "    [255, 112, 0],\n",
    "    [143, 255, 0],\n",
    "    [82, 0, 255],\n",
    "    [163, 255, 0],\n",
    "    [255, 235, 0],\n",
    "    [8, 184, 170],\n",
    "    [133, 0, 255],\n",
    "    [0, 255, 92],\n",
    "    [184, 0, 255],\n",
    "    [255, 0, 31],\n",
    "    [0, 184, 255],\n",
    "    [0, 214, 255],\n",
    "    [255, 0, 112],\n",
    "    [92, 255, 0],\n",
    "    [0, 224, 255],\n",
    "    [112, 224, 255],\n",
    "    [70, 184, 160],\n",
    "    [163, 0, 255],\n",
    "    [153, 0, 255],\n",
    "    [71, 255, 0],\n",
    "    [255, 0, 163],\n",
    "    [255, 204, 0],\n",
    "    [255, 0, 143],\n",
    "    [0, 255, 235],\n",
    "    [133, 255, 0],\n",
    "    [255, 0, 235],\n",
    "    [245, 0, 255],\n",
    "    [255, 0, 122],\n",
    "    [255, 245, 0],\n",
    "    [10, 190, 212],\n",
    "    [214, 255, 0],\n",
    "    [0, 204, 255],\n",
    "    [20, 0, 255],\n",
    "    [255, 255, 0],\n",
    "    [0, 153, 255],\n",
    "    [0, 41, 255],\n",
    "    [0, 255, 204],\n",
    "    [41, 0, 255],\n",
    "    [41, 255, 0],\n",
    "    [173, 0, 255],\n",
    "    [0, 245, 255],\n",
    "    [71, 0, 255],\n",
    "    [122, 0, 255],\n",
    "    [0, 255, 184],\n",
    "    [0, 92, 255],\n",
    "    [184, 255, 0],\n",
    "    [0, 133, 255],\n",
    "    [255, 214, 0],\n",
    "    [25, 194, 194],\n",
    "    [102, 255, 0],\n",
    "    [92, 0, 255],\n",
    "])\n",
    "\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\", cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\", cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "image = load_image('../5.0_pictures/kellogsfrosties_removebg_preview.jpg').convert(\"RGB\")\n",
    "\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = image_segmentor(pixel_values)\n",
    "\n",
    "seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "\n",
    "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[seg == label, :] = color\n",
    "\n",
    "color_seg = color_seg.astype(np.uint8)\n",
    "\n",
    "newimage = Image.fromarray(color_seg)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-seg\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"honey flavoured cornflakes package\", image, num_inference_steps=20).images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(newimage)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Segmenation Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e4f3d-aa06-43f2-967b-f4e42b4d22da",
   "metadata": {},
   "source": [
    "<a id=\"normalmap\"></a>\r\n",
    "## 8. Normal Map Version\n",
    "This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92772da-5673-4a30-a3c1-ac8ece3498dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png\").convert(\"RGB\")\n",
    "\n",
    "depth_estimator = pipeline(\"depth-estimation\", model =\"Intel/dpt-hybrid-midas\" )\n",
    "\n",
    "image = depth_estimator(image)['predicted_depth'][0]\n",
    "\n",
    "image = image.numpy()\n",
    "\n",
    "image_depth = image.copy()\n",
    "image_depth -= np.min(image_depth)\n",
    "image_depth /= np.max(image_depth)\n",
    "\n",
    "bg_threhold = 0.4\n",
    "\n",
    "x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "x[image_depth < bg_threhold] = 0\n",
    "\n",
    "y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "y[image_depth < bg_threhold] = 0\n",
    "\n",
    "z = np.ones_like(x) * np.pi * 2.0\n",
    "\n",
    "image = np.stack([x, y, z], axis=2)\n",
    "image /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\n",
    "image = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"fusing/stable-diffusion-v1-5-controlnet-normal\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"cute little toy\", \n",
    "              image, \n",
    "              num_inference_steps=20\n",
    "             ).images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png\").convert(\"RGB\"))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Segmenation Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb12d7a-cca3-4d5e-a87a-3426d54f3cf6",
   "metadata": {},
   "source": [
    "####  Normal Map - Kellogg's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200a827-b9e3-4733-9b1b-b002d5be86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image = load_image('../5.0_pictures/kellogsfrosties_removebg_preview.jpg').convert(\"RGB\")\n",
    "\n",
    "depth_estimator = pipeline(\"depth-estimation\", model =\"Intel/dpt-hybrid-midas\" )\n",
    "\n",
    "image = depth_estimator(image)['predicted_depth'][0]\n",
    "\n",
    "image = image.numpy()\n",
    "\n",
    "image_depth = image.copy()\n",
    "image_depth -= np.min(image_depth)\n",
    "image_depth /= np.max(image_depth)\n",
    "\n",
    "bg_threhold = 0.4\n",
    "\n",
    "x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "x[image_depth < bg_threhold] = 0\n",
    "\n",
    "y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "y[image_depth < bg_threhold] = 0\n",
    "\n",
    "z = np.ones_like(x) * np.pi * 2.0\n",
    "\n",
    "image = np.stack([x, y, z], axis=2)\n",
    "image /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\n",
    "image = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"fusing/stable-diffusion-v1-5-controlnet-normal\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "images = pipe(\"chocolate cereals\", \n",
    "              image, \n",
    "              num_inference_steps=20\n",
    "             ).images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(load_image('../5.0_pictures/kellogsfrosties_removebg_preview.jpg').convert(\"RGB\"))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Segmenation Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Generated Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee7ef8-f857-4d89-8463-dcbcd176f74d",
   "metadata": {},
   "source": [
    "<a id=\"dreamboothcontrolnet\"></a>\n",
    "## 09. Dreambooth x ControlNet\n",
    "\n",
    "Documentation:\n",
    "https://huggingface.co/blog/controlnet\n",
    "\n",
    "Dreambooth Model:\n",
    "https://huggingface.co/sd-dreambooth-library/mr-potato-head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423929e-4ffd-42c9-adab-52fae007d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ")\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "model_id = \"sd-dreambooth-library/mr-potato-head\"\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=torch.float16, \n",
    "    cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "generator = torch.manual_seed(10)\n",
    "prompt = \" a photo of sks mr potato head, best quality, extremely detailed\"\n",
    "images = pipe(\n",
    "    prompt,\n",
    "    canny_image,\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, low quality\",\n",
    "    num_inference_steps=20,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "# Bilder mit matplotlib darstellen\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Ursprüngliches Bild anzeigen\n",
    "axes[0].imshow(load_image(\"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Canny Image anzeigen\n",
    "axes[1].imshow(canny_image)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Canny Image')\n",
    "\n",
    "# #Generierte Bild anzeigen\n",
    "axes[2].imshow(images)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Dreambooth x ControlNet Generation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeea82f-9c1b-48c8-958a-153e51e05826",
   "metadata": {},
   "source": [
    "<a id=\"combining\"></a>\n",
    "\n",
    "## 10. Combining Multible Conditionings\n",
    "Multiple ControlNet conditionings can be combined for a single image generation. Pass a list of ControlNets to the pipeline's constructor and a corresponding list of conditionings to __call__.\n",
    "\n",
    "Here the **canny** conditioning will be combined with the **open pose** conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b916d-35c9-4bd8-95e9-7702356f0852",
   "metadata": {},
   "source": [
    "#### Canny Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cacde3-1ace-49e2-add5-07567508f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c205f06-8978-423c-a410-2c964f9f685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Image\n",
    "image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n",
    ")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c21641-876a-4252-9027-191687f68f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Canny Image\n",
    "\n",
    "canny_image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n",
    ")\n",
    "canny_image = np.array(canny_image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "canny_image = cv2.Canny(canny_image, low_threshold, high_threshold)\n",
    "\n",
    "# zero out middle columns of image where pose will be overlayed\n",
    "zero_start = canny_image.shape[1] // 4\n",
    "zero_end = zero_start + canny_image.shape[1] // 2\n",
    "canny_image[:, zero_start:zero_end] = 0\n",
    "\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "canny_image = Image.fromarray(canny_image)\n",
    "canny_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c421ee-2d86-4802-acc2-2182cf38525b",
   "metadata": {},
   "source": [
    "#### Open Pose Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5329cd7-f8ff-4993-b0ee-e7fc360543ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from controlnet_aux import OpenposeDetector\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4060b7-7547-4f97-8c53-7e01b01514e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Image\n",
    "image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n",
    ")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049cee6-c4d6-4748-a592-b85309d613b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Pose Image\n",
    "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\", cache_dir=\"/cluster/user/ehoemmen/.cache\")\n",
    "\n",
    "openpose_image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n",
    ")\n",
    "openpose_image = openpose(openpose_image)\n",
    "\n",
    "openpose_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00603888-81f5-407e-90fd-cf6c2c2684f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining Both\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "\n",
    "controlnet = [\n",
    "    ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"),\n",
    "    ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"),\n",
    "]\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, cache_dir=\"/cluster/user/ehoemmen/.cache\"\n",
    ")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "prompt = \"a giant standing in a fantasy landscape, best quality\"\n",
    "negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(1)\n",
    "\n",
    "images = [openpose_image, canny_image]\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    images,\n",
    "    num_inference_steps=20,\n",
    "    generator=generator,\n",
    "    negative_prompt=negative_prompt,\n",
    "    controlnet_conditioning_scale=[1.0, 0.8],\n",
    ").images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94901af0-0fc9-43e8-b0cf-35b5a8ce635b",
   "metadata": {},
   "source": [
    "<a id=\"keyfindings\"></a>\n",
    "\n",
    "## 11. Key Findings\n",
    "\n",
    "The various conditionings via ControlNET make it possible to retain certain aspects and structures of an image when generating a new image. If the respective conditionings are used correctly and for the appropriate application, very good results can be achieved. However, the majority of ControlNETs are not suitable for food packaging.\n",
    "\n",
    "For packaging development applications, the **Canny Edge** is the most promising conditioning method. Especially with regard to numerous design elements that are predefined in the packaging design (e.g. logo, design structure, ...) or should only be modified to a limited extent (e.g. color change), **Canny Edge** can be used to exert good control over the process. \n",
    "\n",
    "It is also interesting to note that ControlNETs can only be applied to certain **sections of the image**. This limitation means that control can only be applied to the desired areas of the image, while other areas can be freely generated by the model. It is conceivable that only the prepared product shown on the packaging could be taken into account via the ControlNET in order to depict a chocolate cake with the same shape and structure instead of an apple pie.\n",
    "\n",
    "It is also possible to **combine ControlNET with fine-tuning methods** such as Dreambooth. With **ControlNET x Dreambooth** very individual images can be created that correspond to your own specifications. I have not yet tested this method with my own Dreambooth training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
